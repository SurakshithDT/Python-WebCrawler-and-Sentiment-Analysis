{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7871056-629c-4ac3-86d4-4b319acbab96",
   "metadata": {},
   "source": [
    "                                                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5159f37-69fd-440a-97f2-b878704f2930",
   "metadata": {},
   "source": [
    "# Black Coffer Assignment (Data Science)\n",
    "## Article Crawler and Text Analysis \n",
    "### &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;By Surakshith D.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac18eec-04dd-4073-8ed0-323ca07ac6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (2.1.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install pandas requests beautifulsoup4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9440573-0966-4513-b865-7d8f68b349cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles have been extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Loading the Excel file containing url ids and links\n",
    "df = pd.read_excel('input.xlsx') #to be changed according to the file location\n",
    "#since i am using jupyter notebook , i have the file locally available(same folder) hence complete path not needed\n",
    "\n",
    "def extract_article_text(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the article title \n",
    "        title = soup.find('title').get_text()\n",
    "\n",
    "        # Extract the article text \n",
    "        article_body = soup.find('body') \n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all('p')\n",
    "            article_text = '\\n'.join([para.get_text() for para in paragraphs])\n",
    "\n",
    "            return title + '\\n\\n' + article_text\n",
    "    return None\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Extract the article text\n",
    "    article_text = extract_article_text(url)\n",
    "    \n",
    "    if article_text:\n",
    "        # Save the article text to a text file with the URL_ID as the filename\n",
    "        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(article_text)\n",
    "\n",
    "print(\"Articles have been extracted and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c309ea08-c7f9-4db0-a062-bc9fbdd19843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: textstat in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: pyphen in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from textstat) (0.15.0)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91636\\anaconda3\\conda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk textstat openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec94bf7d-c86a-4ce8-92d0-f6a2cc38bb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91636\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91636\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analysis complete and results saved to Excel file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import textstat\n",
    "import re\n",
    "\n",
    "# Download NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') # i have downloaded it locally, path should be adjusted according to file storage path\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('Output Data Structure.xlsx') # path should be adjusted according to file storage path\n",
    "\n",
    "# Load stop words, positive words, and negative words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_dir = 'StopWords'\n",
    "stop_words_files = [f for f in os.listdir(stop_words_dir) if f.endswith('.txt')]\n",
    "\n",
    "for file in stop_words_files:\n",
    "    with open(os.path.join(stop_words_dir, file), 'r') as f:\n",
    "        stop_words.update(f.read().split())\n",
    "\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "with open('positive-words.txt', 'r') as f:\n",
    "    positive_words.update(f.read().split())\n",
    "\n",
    "with open('negative-words.txt', 'r') as f:\n",
    "    negative_words.update(f.read().split())\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate sentiment scores\n",
    "def calculate_sentiment(tokens):\n",
    "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "    negative_score = sum(1 for word in tokens if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "# Function to count complex words (more than two syllables)\n",
    "def count_complex_words(text):\n",
    "    words = text.split()\n",
    "    complex_words = 0\n",
    "    for word in words:\n",
    "        syllable_count = textstat.syllable_count(word)\n",
    "        if syllable_count > 2:\n",
    "            complex_words += 1\n",
    "    return complex_words\n",
    "\n",
    "# Function to calculate readability and other metrics\n",
    "def calculate_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    word_count = len(cleaned_words)\n",
    "    sentence_count = len(sentences)\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    complex_words = count_complex_words(' '.join(cleaned_words))\n",
    "    syllable_count = sum(textstat.syllable_count(word) for word in cleaned_words)\n",
    "    syllable_count_per_word = syllable_count / word_count if word_count > 0 else 0\n",
    "    percentage_complex_words = complex_words / word_count if word_count > 0 else 0\n",
    "    avg_word_length = sum(len(word) for word in cleaned_words) / word_count if word_count > 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
    "    \n",
    "    return (avg_sentence_length, complex_words, word_count, syllable_count_per_word,\n",
    "            fog_index, personal_pronouns, avg_word_length, percentage_complex_words, sentence_count)\n",
    "\n",
    "# Process each text file and fill the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    text_file = f'{url_id}.txt'\n",
    "    \n",
    "    if os.path.exists(text_file):\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Clean the text and calculate sentiment scores\n",
    "        tokens = clean_text(text)\n",
    "        positive_score, negative_score, polarity_score, subjectivity_score = calculate_sentiment(tokens)\n",
    "        \n",
    "        # Calculate readability and other metrics\n",
    "        (avg_sentence_length, complex_words, word_count, syllable_count_per_word,\n",
    "         fog_index, personal_pronouns, avg_word_length, percentage_complex_words, sentence_count) = calculate_metrics(text)\n",
    "        \n",
    "        # Fill the DataFrame with calculated values\n",
    "        df.at[index, 'POSITIVE SCORE'] = positive_score\n",
    "        df.at[index, 'NEGATIVE SCORE'] = negative_score\n",
    "        df.at[index, 'POLARITY SCORE'] = polarity_score\n",
    "        df.at[index, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
    "        df.at[index, 'AVG SENTENCE LENGTH'] = avg_sentence_length\n",
    "        df.at[index, 'COMPLEX WORDS'] = complex_words\n",
    "        df.at[index, 'WORD COUNT'] = word_count\n",
    "        df.at[index, 'SYLLABLE COUNT PER WORD'] = syllable_count_per_word\n",
    "        df.at[index, 'FOG INDEX'] = fog_index\n",
    "        df.at[index, 'PERSONAL PRONOUNS'] = personal_pronouns\n",
    "        df.at[index, 'AVG WORD LENGTH'] = avg_word_length\n",
    "        df.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n",
    "        df.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = avg_sentence_length\n",
    "        df.at[index, 'COMPLEX WORD COUNT'] = complex_words\n",
    "        df.at[index, 'SYLLABLE PER WORD'] = syllable_count_per_word\n",
    "\n",
    "# Save the updated DataFrame back to the Excel file\n",
    "df.to_excel('Output Data Structure.xlsx', index=False)\n",
    "print(\"Text analysis complete and results saved to Excel file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e68c0-3b91-4883-bcd4-238cdb6d2b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
